{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig, DistilBertModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers import TrainingArguments\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT for multi-class text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6838"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'optimism',\n",
       " 'pessimism',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'trust']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the labels dataset for inference\n",
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"Tweet\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/4275 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 383.8879315853119\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30472490191459656, 'eval_f1': 0.7099779357685707, 'eval_roc_auc': 0.8040478769533747, 'eval_accuracy': 0.28103837471783294, 'eval_runtime': 2.6522, 'eval_samples_per_second': 334.06, 'eval_steps_per_second': 41.852, 'epoch': 5.0}\n",
      "Evaluation time: 2.654778003692627\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(trainer.evaluate())\n",
    "print(f\"Evaluation time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fine-tuned model\n",
    "fine_tuned_path = \"./bert-finetuned-sem_eval-english\"\n",
    "trainer.save_model(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation of BERT Model Using DistilBERT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationTrainingArguments(TrainingArguments):\n",
    "  def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "    #*args allows us to pass a variable number of non-keyword arguments to a Python function.\n",
    "    #**kwargs stands for keyword arguments. The only difference from args is that it uses keywords and returns the values in the form of a dictionary.\n",
    "    super().__init__(*args, **kwargs)\n",
    "    #The super() function is often used with the __init__() method to initialize the attributes of the parent class.\n",
    "    self.alpha = alpha\n",
    "    self.temperature = temperature\n",
    "\n",
    "class KnowledgeDistillationTrainer(Trainer):\n",
    "  def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.teacher_model = teacher_model\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    #Extract cross-entropy loss and logits from student\n",
    "    outputs_student = model(**inputs)\n",
    "    loss_ce = outputs_student.loss\n",
    "    logits_student = outputs_student.logits\n",
    "\n",
    "    # Extract logits from teacher\n",
    "    outputs_teacher = self.teacher_model(**inputs)\n",
    "    logits_teacher = outputs_teacher.logits\n",
    "\n",
    "     #Computing distillation loss by Softening probabilities\n",
    "    loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    #The reduction=batchmean argument in nn.KLDivLoss() specifies that we average the losses over the batch dimension.\n",
    "    loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "                F.log_softmax(logits_student / self.args.temperature, dim=-1),\n",
    "                F.softmax(logits_teacher / self.args.temperature, dim=-1))\n",
    "\n",
    "    # Return weighted student loss\n",
    "    loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
    "    return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model_name = \"distilbert-base-uncased\"\n",
    "student_tokenizer = DistilBertTokenizer.from_pretrained(student_model_name)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from datasets import load_metric\n",
    "#accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "#def compute_metrics(pred):\n",
    "#  predictions, labels = pred\n",
    "#  predictions = np.argmax(predictions, axis=1)\n",
    "#  return accuracy_score.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "student_model = DistilBertForSequenceClassification.from_pretrained(student_model_name, \n",
    "                                                              problem_type=\"multi_label_classification\", \n",
    "                                                              num_labels=len(labels),\n",
    "                                                              id2label=id2label,\n",
    "                                                              label2id=label2id).to(device)\n",
    "\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(fine_tuned_path, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_training_args = KnowledgeDistillationTrainingArguments(\n",
    "    output_dir=\"./student_model\", \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=5, \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, \n",
    "    alpha=1, \n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=metric_name,\n",
    "    load_best_model_at_end=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4275' max='4275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4275/4275 07:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.326479</td>\n",
       "      <td>0.689673</td>\n",
       "      <td>0.787747</td>\n",
       "      <td>0.265237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.340214</td>\n",
       "      <td>0.690739</td>\n",
       "      <td>0.792089</td>\n",
       "      <td>0.270880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.352306</td>\n",
       "      <td>0.684952</td>\n",
       "      <td>0.787725</td>\n",
       "      <td>0.244921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.364707</td>\n",
       "      <td>0.682781</td>\n",
       "      <td>0.785210</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.369166</td>\n",
       "      <td>0.684120</td>\n",
       "      <td>0.787296</td>\n",
       "      <td>0.238149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets start the training\n",
    "start_time = time.time()\n",
    "distilbert_trainer = KnowledgeDistillationTrainer(student_model,\n",
    "        teacher_model=teacher_model, args=student_training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics, \n",
    "        tokenizer=student_tokenizer)\n",
    "distilbert_trainer.train()\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time For Distillation:  465.87510895729065\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time For Distillation: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34021425247192383, 'eval_f1': 0.6907393760746744, 'eval_roc_auc': 0.7920888880212138, 'eval_accuracy': 0.2708803611738149, 'eval_runtime': 3.5856, 'eval_samples_per_second': 247.101, 'eval_steps_per_second': 30.957, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(distilbert_trainer.evaluate())\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 3.588000535964966\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluation time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Performance to Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model size: 417.682MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in teacher_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in teacher_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "teacher_size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('Teacher model size: {:.3f}MB'.format(teacher_size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model size: 255.443MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in student_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in student_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "student_size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('Student model size: {:.3f}MB'.format(student_size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compression: 1.6x\n"
     ]
    }
   ],
   "source": [
    "print(\"Total compression: {:.1f}x\".format(teacher_size_all_mb / student_size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
